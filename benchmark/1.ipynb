{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attr import define, field\n",
    "from typing import List, Union, ClassVar, Optional, TypeVar\n",
    "import heapq\n",
    "from utils import temporary_file\n",
    "import subprocess\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "import random\n",
    "from enum import Enum\n",
    "from utils import END_LINE_CHARS\n",
    "from pathlib import Path\n",
    "from sequence import Sequence\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "import random \n",
    "import glob\n",
    "import json\n",
    "\n",
    "from typing import TypeVar\n",
    "import glob\n",
    "import parse\n",
    "from datasetconfig import DatasetConfig\n",
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "from dataset import Dataset, DatasetType\n",
    "from labels import BinaryLabel\n",
    "from seqentry import SeqEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = TypeVar('U')\n",
    "CHS_ROOT: Path = Path(\"/home_local/vorontsovie/greco-data/release_7a.2021-10-14/full/CHS/Val_intervals\")\n",
    "BEDTOOLS_PATH: Path = Path(\"/home_local/dpenzar/bedtools2/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedClosestMode(Enum):\n",
    "    UPSTREAM = \"-fu\"\n",
    "    DOWNSTREAM = \"-fd\"\n",
    "    ALL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@define\n",
    "class BedtoolsExecutor:\n",
    "    bedtools_root: Path\n",
    "\n",
    "    @property\n",
    "    def merge_path(self) -> Path:\n",
    "        return self.bedtools_root / 'mergeBed'\n",
    "\n",
    "    @property\n",
    "    def sort_path(self) -> Path:\n",
    "        return self.bedtools_root / 'sortBed'\n",
    "\n",
    "    @property\n",
    "    def subtract_path(self) -> Path:\n",
    "        return self.bedtools_root / 'subtractBed'\n",
    "\n",
    "    @property\n",
    "    def closest_path(self) -> Path:\n",
    "        return self.bedtools_root / 'closestBed'\n",
    "\n",
    "    @property\n",
    "    def flank_path(self) -> Path:\n",
    "        return self.bedtools_root / 'flankBed'\n",
    "\n",
    "    @staticmethod\n",
    "    def run_bedtools_cmd(cmd: str, name: str=\"\"):\n",
    "        out_path = temporary_file()\n",
    "        args = shlex.split(cmd)\n",
    "        with open(out_path, \"w\") as outp:\n",
    "            r = subprocess.run(args, stdout=outp, stderr=subprocess.PIPE, text=True)\n",
    "        if r.stderr:\n",
    "            raise Exception(f\"Bedtools {name} returned error: {r.stderr}\")\n",
    "        return out_path\n",
    "\n",
    "    def merge(self, path: Path) -> 'Path':\n",
    "        cmd = f\"{self.merge_path} -i {path}\" # remove peak columns as ambiguous\n",
    "        out_path = self.run_bedtools_cmd(cmd, name='merge')\n",
    "        return out_path\n",
    "\n",
    "    def subtract(self, a: Path, b: Path, full=False) -> Path:\n",
    "        cmd = f\"{self.subtract_path} {'-A' if full else ''} -a {a} -b {b}\"\n",
    "        out_path = self.run_bedtools_cmd(cmd, name='subtract')\n",
    "        return out_path\n",
    "\n",
    "    def closest(self, a: Path, b: Path, how: BedClosestMode) -> Path:\n",
    "        cmd = f\"{self.closest_path} -D ref {how.value} -a {a} -b {b}\"\n",
    "        out_path = self.run_bedtools_cmd(cmd, name='closest')\n",
    "        return out_path\n",
    "\n",
    "    def flank(self, path: Path, genomesizes: Path, size: int) -> Path:\n",
    "        cmd = f\"{self.flank_path} -i {path} -g {genomesizes} -b {size}\"\n",
    "        out_path = self.run_bedtools_cmd(cmd, name=\"flank\")\n",
    "        return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BED_TOOLS_EXECUTOR = BedtoolsExecutor(BEDTOOLS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@define\n",
    "class Genome:\n",
    "    chroms : dict[str, Sequence]\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        return len(self.chroms)\n",
    "\n",
    "    def __getitem__(self, entry : 'BedEntry') -> Sequence:\n",
    "        return self.chroms[entry.chr][entry.start:entry.end]\n",
    "\n",
    "    def cut(self, bed: 'BedData') -> List[Sequence]:\n",
    "        return [self[e] for e in bed.entries]\n",
    "\n",
    "    def chrom_sizes(self) -> dict[str, int]:\n",
    "        return {ch: len(seq.seq) for ch, seq in self.chroms.items()}\n",
    "\n",
    "    def write_bed_genome_file(self, path: Union[str, Path]):\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        ch_sizes = self.chrom_sizes()\n",
    "        with path.open('w') as outp:\n",
    "            for ch, size in ch_sizes.items():\n",
    "                print(ch, size, file=outp, sep=\"\\t\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_dir(cls, dirpath:Path, ext='.fa'):\n",
    "        dt = {}\n",
    "        for chrom in glob.glob(str(dirpath/f\"*{ext}\")):\n",
    "\n",
    "            ch_name = Path(chrom).name.replace(ext, \"\")\n",
    "            seq = Sequence.from_file(chrom)\n",
    "            dt[ch_name] = seq\n",
    "\n",
    "        return cls(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@define(order=True, slots=True)\n",
    "class BedEntry:\n",
    "    chr: str\n",
    "    start: int\n",
    "    end: int = field(eq=False)\n",
    "    peak: Optional[int] = field(default=None, eq=False)\n",
    "\n",
    "    NONE_REPR: ClassVar[str] = '.'\n",
    "    BED_SEP: ClassVar[str] = \"\\t\"\n",
    "\n",
    "    @classmethod\n",
    "    def peak2str(cls, peak: Union[int, None]) -> str:\n",
    "        if peak is None:\n",
    "            return cls.NONE_REPR\n",
    "        else: # isinstance(peak, int):\n",
    "            return str(peak)\n",
    "    \n",
    "    @classmethod\n",
    "    def str2peak(cls, s: str) -> Union[int, None]:\n",
    "        if s == cls.NONE_REPR:\n",
    "            return None\n",
    "        try:\n",
    "            return int(s)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        raise Exception(\"Wrong peak format\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_line(cls, line: str):\n",
    "        fields = line.split(cls.BED_SEP)\n",
    "        if len(fields) < 3:\n",
    "            raise Exception(\"Wrong bed format\")\n",
    "        if len(fields) == 3:\n",
    "            chr, st, end = fields\n",
    "            return cls(chr, int(st), int(end))\n",
    "        # len(fields) >= 4\n",
    "        chr, st, end, peak = fields[0:4]\n",
    "        return cls(chr, int(st), int(end), cls.str2peak(peak))\n",
    "\n",
    "    def to_line(self, include_peak: bool=True) -> str:\n",
    "        fields = [self.chr, str(self.start), str(self.end)]\n",
    "        if include_peak:\n",
    "            fields.append(self.peak2str(self.peak))\n",
    "        return self.BED_SEP.join(fields)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return max(0, self.end - self.start) \n",
    "\n",
    "    def __getitem__(self, ind: int) -> int:\n",
    "        i = self.start + ind\n",
    "        if i >= self.end:\n",
    "            raise IndexError('segment object index out of range')\n",
    "        return i\n",
    "\n",
    "    def truncate(self, shift: int, how: str=\"both\") -> 'BedEntry':\n",
    "        '''\n",
    "        how - ['left', 'rigth', 'both']\n",
    "        '''\n",
    "        s = self.start\n",
    "        if how != \"right\":\n",
    "            s += shift\n",
    "        e = self.end \n",
    "        if how != 'left':\n",
    "            e -= shift\n",
    "        if s >= e:\n",
    "            return BedEntry('', 0, 0, None)\n",
    "\n",
    "        if self.peak is None or self.peak < s or self.peak > e:\n",
    "            peak = None\n",
    "        else:\n",
    "            peak = self.peak\n",
    "        return BedEntry(self.chr, s, e, peak) \n",
    "    \n",
    "    def split(self, ind: int) -> tuple['BedEntry', 'BedEntry']:\n",
    "        m = self[ind]\n",
    "        s1, e1 = self.start, m\n",
    "        s2, e2 = m, self.end\n",
    "\n",
    "        if self.peak is None:\n",
    "            p1, p2 = None, None\n",
    "        else:\n",
    "            if self.peak < m:\n",
    "                p1, p2 = self.peak, None\n",
    "            else:\n",
    "                p1, p2 = None, self.peak\n",
    "\n",
    "        return BedEntry(self.chr, s1, e1, p1), BedEntry(self.chr, s2, e2, p2)\n",
    "\n",
    "    def expand(self, shift: int) -> 'BedEntry':\n",
    "        st, end = self.start, self.end\n",
    "        st -= shift\n",
    "        end += shift\n",
    "        return BedEntry(self.chr, st, end, self.peak)\n",
    "\n",
    "    @classmethod\n",
    "    def from_center(cls, chr: str, cntr: int, radius: int):\n",
    "        st = cntr - radius\n",
    "        end = cntr + 1 + radius\n",
    "        return cls(chr, st, end, cntr)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@define\n",
    "class BedData:\n",
    "    entries : list[BedEntry] = field(repr=False, factory=list)\n",
    "    executor : BedtoolsExecutor = BED_TOOLS_EXECUTOR\n",
    "    sorted: bool = False\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, ind) -> BedEntry:\n",
    "        return self.entries[ind]\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path: Union[Path, str], sort=False, presorted=False, header=False):\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        entries = []\n",
    "        with path.open() as infile:\n",
    "            if header:\n",
    "                infile.readline()\n",
    "            for line in infile:\n",
    "                line = line.rstrip(END_LINE_CHARS)\n",
    "                entry = BedEntry.from_line(line)\n",
    "                entries.append(entry)\n",
    "        self = cls(entries, sorted=sort)\n",
    "        if sort and not presorted:\n",
    "            self.sort()\n",
    "        return self\n",
    "\n",
    "    def sort(self):\n",
    "        self.entries.sort()\n",
    "        self.sorted = True\n",
    "\n",
    "    def write(self, path:Union[Path, str], write_peak: bool=True):\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        with path.open(\"w\") as output:\n",
    "            for e in self.entries:\n",
    "                line = e.to_line(include_peak=write_peak)\n",
    "                print(line, file=output, sep=\"\\t\")\n",
    "\n",
    "    def merge(self) -> 'BedData':\n",
    "        store = temporary_file()\n",
    "        self.write(store)\n",
    "        out_path = temporary_file()\n",
    "        out_path = self.executor.merge(store)\n",
    "        return self.from_file(out_path, presorted=True)\n",
    "\n",
    "    def subtract(self, other: 'BedData', full: bool) -> 'BedData':\n",
    "        tmp1 = temporary_file()\n",
    "        tmp2 = temporary_file()\n",
    "        self.write(tmp1)\n",
    "        other.write(tmp2)\n",
    "        out_path = self.executor.subtract(tmp1, tmp2, full=full)\n",
    "        return self.from_file(out_path, presorted=True)\n",
    "\n",
    "    def closest(self, other: 'BedData', how: BedClosestMode) -> List[int]:\n",
    "        '''\n",
    "        for each feature in self distance to closest feature in other\n",
    "        if no such feature exists - return None\n",
    "        '''\n",
    "        store1 = temporary_file()\n",
    "        store2 = temporary_file()\n",
    "        self.write(store1)\n",
    "        other.write(store2)\n",
    "        out_path = self.executor.closest(store1, store2, how=how)\n",
    "        lst = []\n",
    "        with open(out_path) as inp:\n",
    "            fields = inp.readline().split()\n",
    "            dist_ind = len(fields) - 1\n",
    "            chrom_ind = len(fields) // 2\n",
    "            inp.seek(0)\n",
    "            for line in inp:\n",
    "                fields = line.rstrip(END_LINE_CHARS).split()\n",
    "                chrom = fields[chrom_ind]\n",
    "                if chrom == \".\":\n",
    "                    lst.append(None)\n",
    "                else:\n",
    "                    dist = int(fields[dist_ind])\n",
    "                    if dist < 0 and how is BedClosestMode.DOWNSTREAM:\n",
    "                        lst.append(None)\n",
    "                    else:\n",
    "                        lst.append(dist)\n",
    "        return lst\n",
    "\n",
    "    def flank(self, genome: Union[str, Path, Genome], size: int) -> 'BedData':\n",
    "        if isinstance(genome, Genome):\n",
    "            genomesizes = temporary_file()\n",
    "            genome.write_bed_genome_file(genomesizes)\n",
    "            genome = genomesizes\n",
    "        elif isinstance(genome, str):\n",
    "            genome = Path(genome)\n",
    "        elif not isinstance(genome, Path):\n",
    "            raise Exception(f\"Wrong type of genome argument: {type(genome)}\")\n",
    "        \n",
    "        store = temporary_file()\n",
    "        self.write(store)\n",
    "        out_path = self.executor.flank(store, genome, size)\n",
    "        return BedData.from_file(out_path)\n",
    "\n",
    "    def append(self, e: BedEntry) -> None:\n",
    "        self.entries.append(e)\n",
    "        self.sorted = False\n",
    "\n",
    "    def pop(self, ind) -> 'BedEntry':\n",
    "        return self.entries.pop(ind)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return sum(len(e) for e in self.entries)\n",
    "\n",
    "    def apply(self, fn : Callable[[BedEntry], Optional[BedEntry]]) -> 'BedData':\n",
    "        new_entries = []\n",
    "        for s in self.entries:\n",
    "            s = fn(s)\n",
    "            if s is not None:\n",
    "                new_entries.append(s)\n",
    "        return BedData(new_entries)\n",
    "    \n",
    "    def map(self, fn : Callable[[BedEntry], Optional[U]]) -> list[U]:\n",
    "        lst = []\n",
    "        for s in self.entries:\n",
    "            s = fn(s)\n",
    "            if s is not None:\n",
    "                lst.append(s)\n",
    "        return lst\n",
    "\n",
    "    def filter(self, predicate: Callable[[BedEntry], bool]) -> 'BedData':\n",
    "        flt_entries = []\n",
    "        for s in self.entries:\n",
    "            if predicate(s):\n",
    "                flt_entries.append(s)\n",
    "        return BedData(flt_entries)\n",
    "\n",
    "    def global2local(self, global_ind: int) -> tuple[int, int]:\n",
    "        ind = global_ind\n",
    "        for si, e in enumerate(self.entries):\n",
    "            if ind < len(e):\n",
    "                break\n",
    "            ind = ind - len(e)\n",
    "        else: # no break\n",
    "            raise IndexError(\"global segmentset index is out of range\")\n",
    "        return si, ind\n",
    "\n",
    "    def retrieve(self, global_ind: int) -> int:\n",
    "        si, ind = self.global2local(global_ind)\n",
    "        return self.entries[si][ind]\n",
    "    \n",
    "    def sample(self, k: int) -> 'BedData':\n",
    "        k = min(k, len(self))\n",
    "        return BedData(random.sample(self.entries, k))\n",
    "\n",
    "    def sample_shades(self, seg_size, k: int=1) -> list[BedEntry]:\n",
    "        shift = seg_size // 2\n",
    "        other = deepcopy(self)\n",
    "        smpls = []\n",
    "        for _ in range(k):        \n",
    "            n = other.size()\n",
    "            if n == 0:\n",
    "                break\n",
    "\n",
    "            g_ind = random.sample(range(n), k=1)[0]\n",
    "            si, ind = other.global2local(g_ind)\n",
    "            entry = other.pop(si)\n",
    "            coord = entry[ind]\n",
    "            \n",
    "            new_entry = BedEntry.from_center(entry.chr, coord, shift)\n",
    "            smpls.append(new_entry)\n",
    "            \n",
    "            en1, en2 = entry.split(ind)\n",
    "            en1 = en1.truncate(shift, how='right')\n",
    "            en2 = en2.truncate(shift, how=\"left\")\n",
    "            if len(en1) > 0:\n",
    "                other.append(en1)\n",
    "            if len(en2) > 0:\n",
    "                other.append(en2)\n",
    "        return smpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_bed(beds: List[BedData],  sort=True) -> BedData:\n",
    "    if any(not x.sorted for x in beds):\n",
    "        entries = []\n",
    "        for b in beds:\n",
    "            entries.extend(b.entries)\n",
    "        if sort:\n",
    "            entries.sort()\n",
    "        return BedData(entries, sorted=sort)\n",
    "    entries = list(heapq.merge(*(b.entries for b in beds)))\n",
    "    return BedData(entries, sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import List\n",
    "\n",
    "@define\n",
    "class UniqueTagger:\n",
    "    parts: typing.Sequence[str]\n",
    "    vocabularies: dict[str, list[str]] = field(repr=False)\n",
    "    used_tags: set[str] = field(repr=False, factory=set) \n",
    "    max_occupancy: float = 0.5\n",
    "    seed: int = 777\n",
    "    max_size: int = field(init=False)\n",
    "    random_generator: random.Random = field(init=False, repr=False)\n",
    "\n",
    "\n",
    "    DEFAULT_PARTS: ClassVar[typing.Sequence[str]] = (\"adj\", \"adj\", \"nat\", \"ani\")\n",
    "    DEFAULT_PARTS_PATH: ClassVar[dict[str, Path]] = {'adj': Path(\"adjectives.txt\"), 'nat': Path('nations.txt'), 'ani': Path('animals.txt')}\n",
    "\n",
    "    PARTS_FIELD: ClassVar[str]=\"PARTS\"\n",
    "    VOC_FIELD: ClassVar[str] = \"VOC\"\n",
    "    TAGS_FIELD: ClassVar[str] = \"USED_TAGS\"\n",
    "    PARTS_SEP: ClassVar[str] = '-'\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.max_size = self._calc_maxsize()\n",
    "        self.random_generator = random.Random(self.seed)\n",
    "\n",
    "    @classmethod\n",
    "    def make(cls, parts: typing.Sequence[str], parts_path: dict[str, Path]):\n",
    "        dt = {}\n",
    "        for part in parts:\n",
    "            with parts_path[part].open() as inp:\n",
    "                voc =  [line.rstrip(END_LINE_CHARS) for line in inp]\n",
    "                dt[part] = voc\n",
    "        return cls(parts, dt)\n",
    "\n",
    "    @classmethod\n",
    "    def default(cls):\n",
    "        return cls.make(cls.DEFAULT_PARTS, cls.DEFAULT_PARTS_PATH)        \n",
    "\n",
    "    def write(self, path: Union[Path, str]):\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        with path.open('w') as store:\n",
    "            dt = {self.PARTS_FIELD: self.parts,\n",
    "                  self.VOC_FIELD: self.vocabularies,\n",
    "                  self.TAGS_FIELD: list(self.used_tags)}\n",
    "            json.dump(dt, store, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Union[Path, str]) -> 'UniqueTagger':\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        with path.open() as inp:\n",
    "            dt = json.load(inp)\n",
    "        parts = dt[cls.PARTS_FIELD]\n",
    "        if not isinstance(parts, list):\n",
    "            raise Exception('Wrong format')\n",
    "        vocs = dt[cls.VOC_FIELD]\n",
    "        if not isinstance(vocs, dict):\n",
    "            raise Exception('Wrong format')\n",
    "        tags = dt[cls.TAGS_FIELD]\n",
    "        if not isinstance(tags, list):\n",
    "            raise Exception('Wrong format')\n",
    "\n",
    "        return cls(tuple(parts), vocs, set(tags)) \n",
    "    \n",
    "    def _non_unique_tag(self) -> str:\n",
    "        tag_prts = [self.random_generator.choice(self.vocabularies[p]) for p in self.parts]\n",
    "        tag = self.PARTS_SEP.join(tag_prts)\n",
    "        return tag\n",
    "    \n",
    "    def tag(self) -> str:\n",
    "        if len(self.used_tags) >= self.max_size:\n",
    "            raise Exception(\"Max size for fast random generation reached\")\n",
    "        while (tag := self._non_unique_tag()) in self.used_tags:\n",
    "            pass\n",
    "        self.used_tags.add(tag)\n",
    "        return tag\n",
    "    \n",
    "    def _calc_maxsize(self) -> int:\n",
    "        cnt = 1\n",
    "        for p in self.parts:\n",
    "            cnt *= len(self.vocabularies[p])\n",
    "        return int(cnt * self.max_occupancy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "from contextlib import contextmanager\n",
    "#https://gist.github.com/tylerneylon\n",
    "class RWLock(object):\n",
    "    \"\"\" RWLock class; this is meant to allow an object to be read from by\n",
    "        multiple threads, but only written to by a single thread at a time. See:\n",
    "        https://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock\n",
    "        Usage:\n",
    "            my_obj_rwlock = RWLock()\n",
    "            # When reading from my_obj:\n",
    "            with my_obj_rwlock.r_locked():\n",
    "                do_read_only_things_with(my_obj)\n",
    "            # When writing to my_obj:\n",
    "            with my_obj_rwlock.w_locked():\n",
    "                mutate(my_obj)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.w_lock = Lock()\n",
    "        self.num_r_lock = Lock()\n",
    "        self.num_r = 0\n",
    "\n",
    "    def r_acquire(self):\n",
    "        self.num_r_lock.acquire()\n",
    "        self.num_r += 1\n",
    "        if self.num_r == 1:\n",
    "            self.w_lock.acquire()\n",
    "        self.num_r_lock.release()\n",
    "\n",
    "    def r_release(self):\n",
    "        assert self.num_r > 0\n",
    "        self.num_r_lock.acquire()\n",
    "        self.num_r -= 1\n",
    "        if self.num_r == 0:\n",
    "            self.w_lock.release()\n",
    "        self.num_r_lock.release()\n",
    "\n",
    "    @contextmanager\n",
    "    def r_locked(self):\n",
    "        try:\n",
    "            self.r_acquire()\n",
    "            yield\n",
    "        finally:\n",
    "            self.r_release()\n",
    "\n",
    "    def w_acquire(self):\n",
    "        self.w_lock.acquire()\n",
    "\n",
    "    def w_release(self):\n",
    "        self.w_lock.release()\n",
    "\n",
    "    @contextmanager\n",
    "    def w_locked(self):\n",
    "        try:\n",
    "            self.w_acquire()\n",
    "            yield\n",
    "        finally:\n",
    "            self.w_release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "\n",
    "@define\n",
    "class SeqDB:\n",
    "    seq2tag: dict[str, str] = field(factory=dict)\n",
    "    tag2seq: dict[str, str] = field(factory=dict)\n",
    "    tagger: UniqueTagger = field(factory=UniqueTagger.default)\n",
    "    lock: RWLock = RWLock() \n",
    "    \n",
    "    def add(self, seq: Union[Sequence, str]) -> str:\n",
    "        if isinstance(seq, Sequence):\n",
    "            seq = seq.seq\n",
    "        tag = self.get_tag(seq)\n",
    "        if tag is None:\n",
    "            with self.lock.w_locked(): \n",
    "                tag = self.tagger.tag()\n",
    "                self.seq2tag[seq] = tag\n",
    "                self.tag2seq[tag] = seq\n",
    "        return tag\n",
    "\n",
    "    def get_tag(self, seq: Union[Sequence, str]) -> Optional[str]:\n",
    "        if isinstance(seq, Sequence):\n",
    "            seq = seq.seq\n",
    "        with self.lock.r_locked():\n",
    "            tag = self.seq2tag.get(seq)\n",
    "        return tag\n",
    "\n",
    "    def get_seq(self, tag: str) -> Sequence:\n",
    "        with self.lock.r_locked():\n",
    "            seq = self.tag2seq[tag]\n",
    "        return Sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "from experiment import ExperimentType\n",
    "from abc import abstractproperty, abstractmethod, ABCMeta\n",
    "from math import ceil\n",
    "\n",
    "class SubProtocol(metaclass=ABCMeta):\n",
    "\n",
    "    @abstractproperty\n",
    "    def data_type(self) -> ExperimentType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self, cfgs: List[DatasetConfig]):\n",
    "        '''\n",
    "        prepare data given cgfs files of all datasets of specified datatype\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def process(self, cfg: Union[List[DatasetConfig], DatasetConfig]) -> 'Dataset':\n",
    "        '''\n",
    "        create dataset from cfg file using it's data and data aquired during\n",
    "        preprocess stage. Data from other cfg should not be used \n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exceptions import WrongDatasetTypeException\n",
    "from typing import Iterator\n",
    "from utils import undict\n",
    "\n",
    "\n",
    "@define\n",
    "class UDataset(metaclass=ABCMeta):\n",
    "    name: str\n",
    "    type: DatasetType\n",
    "    tf_name: str\n",
    "    entries: Optional[List[SeqEntry]] = field(repr=False)\n",
    "    metainfo: dict\n",
    "    disk_path: Path  \n",
    "\n",
    "    SEQUENCE_FIELD: ClassVar[str] = \"sequence\"\n",
    "    LABEL_FIELD: ClassVar[str] = \"label\"\n",
    "    TAG_FIELD: ClassVar[str] = \"tag\"\n",
    "    NO_INFO_VALUE: ClassVar[str] = \"NoInfo\"\n",
    "\n",
    "\n",
    "    def closed(self) -> bool:\n",
    "        return self.entries is None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.closed():\n",
    "            raise Exception(\"Operation on closed dataset\")\n",
    "        return len(self.entries) # type: ignore\n",
    "\n",
    "    def __iter__(self) -> Iterator[SeqEntry]:\n",
    "        if self.closed():\n",
    "            raise Exception(\"Operation on closed dataset\")\n",
    "        return iter(self.entries) # type: ignore\n",
    "\n",
    "    def __getitem__(self, ind: int) -> SeqEntry:\n",
    "        if self.closed():\n",
    "            raise Exception(\"Operation on closed dataset\")\n",
    "        return self.entries[ind] # type: ignore\n",
    "    \n",
    "    def get_fields(self) -> list[str]:\n",
    "        if self.type is DatasetType.TRAIN:\n",
    "            return self.get_train_fields()\n",
    "        if self.type is DatasetType.TEST:\n",
    "            return self.get_test_fields()\n",
    "        if self.type is DatasetType.VALIDATION:\n",
    "            return self.get_valid_fields()\n",
    "        if self.type is DatasetType.FULL:\n",
    "            return self.get_full_fields()\n",
    "        raise WrongDatasetTypeException(f\"{self.type}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_train_fields(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_test_fields(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_valid_fields(self):\n",
    "        return self.get_train_fields()\n",
    "    \n",
    "    def get_full_fields(self):\n",
    "        return self.get_train_fields()\n",
    "\n",
    "    def get(self, key, default):\n",
    "        try:\n",
    "            val = getattr(self, key)\n",
    "        except AttributeError:\n",
    "            val = self.metainfo.get(key, default)\n",
    "        return val\n",
    "    \n",
    "    def to_tsv(self,\n",
    "               path: Path,\n",
    "               ds_fields: Optional[List[str]] = None):\n",
    "        fields = self.get_fields()\n",
    "        \n",
    "        if ds_fields:\n",
    "           ds_values = [self.get(fl, self.NO_INFO_VALUE) for fl in ds_fields]\n",
    "\n",
    "        with open(path, \"w\") as out:\n",
    "            if ds_fields:\n",
    "                header = \"\\t\".join(fields + ds_fields)\n",
    "            else:\n",
    "                header = \"\\t\".join(fields)\n",
    "            print(header, file=out)\n",
    "            for en in self.entries:\n",
    "                values = [en.get(fld, self.NO_INFO_VALUE) for fld in fields]  \n",
    "                if ds_fields:\n",
    "                    values.extend(ds_values) # type: ignore\n",
    "                print(\"\\t\".join(values), file=out)\n",
    "                    \n",
    "    def to_json(self, path, mode: Optional[DatasetType] = DatasetType.TEST):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_canonical_format(self, path, mode: Optional[DatasetType] = DatasetType.TEST):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def split(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Union[Path, str]) -> 'Dataset':\n",
    "        raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " name: str\n",
    "    type: DatasetType\n",
    "    tf_name: str\n",
    "    entries: Optional[List[SeqEntry]] = field(repr=False)\n",
    "    metainfo: dict\n",
    "    disk_path: Path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChIPSeqDataset(Dataset):\n",
    "    def get_train_fields(self) -> list[str]:\n",
    "        return [self.TAG_FIELD, self.SEQUENCE_FIELD,  self.LABEL_FIELD]\n",
    "    \n",
    "    def get_test_fields(self):\n",
    "         return [self.TAG_FIELD, self.SEQUENCE_FIELD]\n",
    "   \n",
    "    def to_canonical_format(self, path):\n",
    "        return self.to_tsv(path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: Union[Path, str], tp: DatasetType, fmt: str) -> 'ChIPSeqDataset':\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        # TODO: fix saving dataset (storing name, tf_name and metainfo in file)\n",
    "        # TODO: add other fmts, maybe using class abstraction\n",
    "        if fmt != \".tsv\":\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        self = cls(name=\"\", tf_name=\"\", entries=[], type=tp, metainfo={})\n",
    "        fieldsname = self.get_fields()\n",
    "        with path.open() as inp:\n",
    "            _ = inp.readline() # header\n",
    "            for line in inp:\n",
    "                fields = line.split()\n",
    "                dt = dict(zip(fieldsname, fields))\n",
    "                entry = SeqEntry(**dt)\n",
    "                self.entries.append(entry)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativesSampler(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def sample(self, prot: 'ChIPSeqIrisProtocol', cfg: DatasetConfig) -> BedData:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# TODO: move sampling functionality and args directly to samplers \n",
    "class ForeignPeakSampler(NegativesSampler):\n",
    "    def sample(self, prot: 'ChIPSeqIrisProtocol', cfg: DatasetConfig) -> BedData:\n",
    "        return prot.get_foreign_peaks(cfg)\n",
    "\n",
    "\n",
    "class ShadesSampler(NegativesSampler):\n",
    "    def sample(self, prot: 'ChIPSeqIrisProtocol', cfg: DatasetConfig) -> BedData:\n",
    "        return prot.get_shades(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@define\n",
    "class ChIPSeqIrisProtocol(SubProtocol):   \n",
    "    mx_dist: int\n",
    "    peak_size: int\n",
    "    shades_per_peak: int\n",
    "    negative_tf_ratio: int \n",
    "    genome: Genome = field(repr=False)\n",
    "    root: Path\n",
    "    db: SeqDB = field(factory=SeqDB)\n",
    "    \n",
    "    TF_MERGE_DIR_NAME: ClassVar[str] = \"TF_MERGE\"\n",
    "    TF_FOREIGN_DIR_NAME: ClassVar[str] = \"TF_FOREIGN\"\n",
    "    REAL_PEAKS_FILE_NAME: ClassVar[str] = \"real.bed\"\n",
    "    ALL_PEAKS_FILE_NAME: ClassVar[str] = \"all.bed\"\n",
    "\n",
    "    MX_DIST_FIELD: ClassVar[str] = \"mx_dist\"\n",
    "    PEAK_SIZE_FIELD: ClassVar[str] = \"peak_size\"\n",
    "    SHADES_PER_PEAK_FIELD: ClassVar[str] = \"shades_per_peak\"\n",
    "    NEGATIVE_TF_RATIO_FIELD: ClassVar[str] = \"negative_ratio\"\n",
    "    ROOT_FIELD: ClassVar[str] = \"root_dir\"\n",
    "    GENOME_FIELD: ClassVar[str] = \"genome\"\n",
    "\n",
    "    DEFAULT_ROOT = Path(\".ChIPSeq\")\n",
    "\n",
    "    @property\n",
    "    def data_type(self) -> ExperimentType:\n",
    "        return ExperimentType.ChIPSeq\n",
    "\n",
    "    @property\n",
    "    def tf_merge_dir(self) -> Path:\n",
    "        return self.root / self.TF_MERGE_DIR_NAME\n",
    "\n",
    "    @property\n",
    "    def tf_foreign_dir(self) -> Path:\n",
    "        return self.root / self.TF_FOREIGN_DIR_NAME\n",
    "\n",
    "    @property\n",
    "    def real_peaks_path(self) -> Path:\n",
    "        return self.root / self.REAL_PEAKS_FILE_NAME\n",
    "   \n",
    "    def mkdirs(self):\n",
    "        self.root.mkdir(exist_ok=True, parents=True)\n",
    "        self.tf_merge_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.tf_foreign_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    def __attrs_post_init__(self):\n",
    "        if self.peak_size % 2 != 1:\n",
    "            raise Exception(f\"Peak size must be odd: {self.peak_size}\")\n",
    "        self.mkdirs()\n",
    "\n",
    "    def tf_merge_path(self, tf_name:str) -> Path:\n",
    "        return self.tf_merge_dir / f\"{tf_name}.bed\"\n",
    "\n",
    "    def tf_foreign_path(self, tf_name:str) -> Path:\n",
    "        return self.tf_foreign_dir / f\"{tf_name}.bed\"\n",
    "\n",
    "    def merge_by_tf(self, cfgs: list[DatasetConfig], real_peaks: BedData):\n",
    "        cfgs.sort(key=lambda x : x.tf)\n",
    "        for tf_name, tf_records in groupby(cfgs, key=lambda x : x.tf):\n",
    "            beds = [BedData.from_file(x.path, header=True) for x in tf_records]\n",
    "            jnd = join_bed(beds)\n",
    "            mrg = jnd.merge()\n",
    "            mrg_path = self.tf_merge_path(tf_name)\n",
    "            mrg.write(mrg_path, write_peak=False)\n",
    "\n",
    "            foreign = real_peaks.subtract(mrg, full=True)\n",
    "            foreign_path = self.tf_foreign_path(tf_name)\n",
    "            foreign.write(foreign_path, write_peak=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def map_peaks(bed: BedData) -> dict[int, BedData]:\n",
    "        dt = defaultdict(BedData)\n",
    "        for e in bed:\n",
    "            dt[e.peak].append(BedEntry(e.chr, e.start, e.end))\n",
    "        return dict(dt)\n",
    "\n",
    "    def sample_shades(self, ds: BedData, tf: BedData) -> BedData:\n",
    "        flanks = ds.flank(self.genome, self.mx_dist + self.peak_size // 2)\n",
    "        sbstr = flanks.subtract(tf, full=False)\n",
    "        dt = self.map_peaks(sbstr)\n",
    "        for peak, sgmnts in dt.items():\n",
    "            sgmnts = sgmnts.apply(lambda s: s.truncate(self.peak_size // 2)).filter(lambda s: len(s) > 0)\n",
    "            dt[peak] = sgmnts\n",
    "        \n",
    "        entries = []\n",
    "        for peak, sgmnts in dt.items():\n",
    "            smpl = sgmnts.sample_shades(seg_size=self.peak_size, k=self.shades_per_peak)\n",
    "            entries.extend(smpl)\n",
    "        return BedData(entries)\n",
    "\n",
    "    def get_shades(self, cfg: DatasetConfig) -> BedData:\n",
    "        ds = BedData.from_file(cfg.path, header=True)\n",
    "        tf_file = self.tf_merge_path(cfg.tf)\n",
    "        tf = BedData.from_file(tf_file, header=True)\n",
    "        return self.sample_shades(ds, tf)\n",
    "    \n",
    "    def get_foreign_peaks(self, cfg: DatasetConfig) -> BedData:\n",
    "        ds = BedData.from_file(cfg.path, header=True)\n",
    "        foreign_path = self.tf_foreign_path(cfg.tf)\n",
    "        foreign = BedData.from_file(foreign_path)\n",
    "        size = len(ds) * self.negative_tf_ratio\n",
    "        return foreign.sample(size)\n",
    "    \n",
    "    def make_real_peaks(self, cfgs: List[DatasetConfig]) -> BedData:\n",
    "        beds = [BedData.from_file(x.path, header=True) for x in configs]\n",
    "        real_bed = join_bed(beds)\n",
    "        real_bed.write(self.real_peaks_path)\n",
    "        return real_bed\n",
    "\n",
    "    def preprocess(self, cfgs: List[DatasetConfig]) -> None:\n",
    "        real_bed = self.make_real_peaks(cfgs)\n",
    "        self.merge_by_tf(cfgs, real_bed)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dt(cls, dt: dict) -> 'ChIPSeqIrisProtocol':\n",
    "        mx_dist=int(dt[cls.MX_DIST_FIELD])\n",
    "        peak_size = int(dt[cls.PEAK_SIZE_FIELD])\n",
    "        shades_per_peak = int(dt[cls.SHADES_PER_PEAK_FIELD])\n",
    "        negative_tf_ratio = int(dt[cls.NEGATIVE_TF_RATIO_FIELD])\n",
    "        genome = Genome.from_dir(dt[cls.GENOME_FIELD])\n",
    "\n",
    "        root  = dt.get(cls.ROOT_FIELD, cls.DEFAULT_ROOT)\n",
    "        root = Path(root)\n",
    "    \n",
    "        return cls(mx_dist=mx_dist,\n",
    "                   peak_size=peak_size,\n",
    "                   shades_per_peak=shades_per_peak,\n",
    "                   negative_tf_ratio=negative_tf_ratio,\n",
    "                   genome=genome,\n",
    "                   root=root)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, path: Union[Path, str]) -> 'ChIPSeqIrisProtocol':\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path)\n",
    "        with path.open() as inp:\n",
    "            dt = json.load(inp)\n",
    "        lower_dt = {key.lower() : value for key, value in dt.items() }\n",
    "        return cls.from_dt(lower_dt)\n",
    "\n",
    "    def get_positives(self, cfg: DatasetConfig) -> list[SeqEntry]:\n",
    "        bed = BedData.from_file(cfg.path, header=True)\n",
    "        seqs = self.genome.cut(bed)\n",
    "        entries = [SeqEntry(s, \n",
    "                        BinaryLabel.POSITIVE,\n",
    "                        self.db.add(s), \n",
    "                        {}) for s in seqs]\n",
    "        return entries\n",
    "\n",
    "    def get_negatives(self, cfg: DatasetConfig, sampler: NegativesSampler) -> List[SeqEntry]:\n",
    "        bed = sampler.sample(self, cfg)\n",
    "        seqs = self.genome.cut(bed)\n",
    "        entries = [SeqEntry(s, \n",
    "                        BinaryLabel.NEGATIVE,\n",
    "                        self.db.add(s), \n",
    "                        {}) for s in seqs]\n",
    "        return entries\n",
    "\n",
    "    def prepare_dataset(self, cfg: DatasetConfig, sampler: NegativesSampler, name: str):\n",
    "        pos = self.get_positives(cfg)\n",
    "        neg = self.get_negatives(cfg, sampler)\n",
    "        return ChIPSeqDataset(name=name, \n",
    "                            tf_name=cfg.tf, \n",
    "                            type=cfg.ds_type, \n",
    "                            entries=pos+neg, \n",
    "                            metainfo=cfg.metainfo)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_shades_ds_name(cfg: DatasetConfig) -> str:\n",
    "        return f\"{cfg.name}_shades\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_foreign_ds_name(cfg: DatasetConfig) -> str:\n",
    "        return f\"{cfg.name}_foreign\"\n",
    "\n",
    "    def process(self, cfg: DatasetConfig) -> list[Dataset]:\n",
    "        shades_ds = self.prepare_dataset(cfg,\n",
    "                                         sampler=ShadesSampler(),\n",
    "                                         name=self.get_shades_ds_name(cfg))\n",
    "        foreign_ds = self.prepare_dataset(cfg,\n",
    "                                          sampler=ForeignPeakSampler(),\n",
    "                                          name=self.get_foreign_ds_name(cfg))\n",
    "        return [shades_ds, foreign_ds]\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peakfls2configs(files: list[Path], \n",
    "                    name_parser= parse.compile(\"{tf_name}.{fl}@{exp_type}@{name}@Peaks.{unique_tag}.{dt_type}.peaks\")) -> list[DatasetConfig]:\n",
    "    all_records = []\n",
    "    for val_fl in files:\n",
    "        val_fl = val_fl.absolute()\n",
    "        data = name_parser.parse(val_fl.name)\n",
    "        if data is None or isinstance(data, parse.Match):\n",
    "            raise Exception(\"Wrong peakfile name format\")\n",
    "        dt = {\n",
    "            \"name\": data[\"name\"],\n",
    "            \"exp_type\": \"ChIPSeq\",\n",
    "            \"tf\": data['tf_name'],\n",
    "            \"ds_type\": data['dt_type'],\n",
    "            \"path\": val_fl,\n",
    "            \"curation_status\": \"accepted\",\n",
    "            \"protocol\": \"iris\",\n",
    "            \"metainfo\": {}\n",
    "        }\n",
    "        all_records.append(DatasetConfig.from_dict(dt))\n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = [Path(x) for x in glob.glob(str(CHS_ROOT / '*.peaks'))]\n",
    "configs = peakfls2configs(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_CHIPSEQ_CONFIG = {\n",
    "    \"mx_dist\": 300,\n",
    "    \"peak_size\": 301,\n",
    "    \"shades_per_peak\": 1,\n",
    "    \"negative_ratio\": 100,\n",
    "    \"genome\": Path(\"/home_local/dpenzar/hg38\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = ChIPSeqIrisProtocol.from_dt(IRIS_CHIPSEQ_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#protocol.preprocess(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lst = protocol.process(configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_DIR = Path('.ChIPSeqDatasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pwmeval import PWMEvalPFMPredictor, PWMEvalPWMPredictor\n",
    "from _benchmark import ModelEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_BENCHMARK_CONFIG = Path(\"/home_local/dpenzar/test_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(OLD_BENCHMARK_CONFIG) as inp:\n",
    "    dt = json.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['datasets'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['scorers'] = dt['scorers'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_CONFIG = Path(\"/home_local/dpenzar/chipseq_config.json\")\n",
    "with BENCHMARK_CONFIG.open('w') as out:\n",
    "    json.dump(dt, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarkconfig import BenchmarkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = BenchmarkConfig.from_json(BENCHMARK_CONFIG).make_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = Path(\"/home_local/dpenzar/models.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6664\n"
     ]
    }
   ],
   "source": [
    "with open(MODELS_PATH, \"r\") as inp:\n",
    "    models = json.load(inp)\n",
    "print(len(models))\n",
    "for mcfg in models:\n",
    "    tf_name = mcfg['tf']\n",
    "    name = mcfg['name']\n",
    "    path = mcfg[\"path\"]\n",
    "    bench.add_pfm(tf_name, name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : add add_dataset method to benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ChIPSeqDataset.load('.ChIPSeqDatasets/THC_0258_foreign.bed', tp=DatasetType.TRAIN, fmt='.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_PROCS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(max_workers=AVAILABLE_PROCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChIPSeqLOGS = Path(\"chipseq_logs\")\n",
    "ChIPSeqLOGS.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_DIR = Path(\".ChIPSeqDatasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset SI0159\n",
      "Submited 0 futures\n",
      "Submited 0 futures\n",
      "Dataset THC_0074\n",
      "Submited 0 futures\n",
      "Submited 0 futures\n",
      "Dataset THC_0246\n",
      "Submited 1 futures\n",
      "Finished for CHAMP1.FL@CHS@silly-champagne-burmese@HughesLab.Homer@Motif1.ppm_sumscore@THC_0246_shades\n",
      "Submited 1 futures\n",
      "Finished for CHAMP1.FL@CHS@silly-champagne-burmese@HughesLab.Homer@Motif1.ppm_sumscore@THC_0246_foreign\n",
      "Dataset THC_0462.Rep-MICHELLE_0314\n",
      "Submited 1 futures\n",
      "Finished for MKX.FL@CHS@droopy-tomato-affenpinscher@HughesLab.Homer@Motif1.ppm_sumscore@THC_0462.Rep-MICHELLE_0314_shades\n",
      "Submited 1 futures\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=43'>44</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSubmited \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(futures)\u001b[39m}\u001b[39;00m\u001b[39m futures\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m ft \u001b[39min\u001b[39;00m as_completed(futures):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=46'>47</a>\u001b[0m     name \u001b[39m=\u001b[39m futures[ft]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000026vscode-remote?line=47'>48</a>\u001b[0m     path \u001b[39m=\u001b[39m ChIPSeqLOGS \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=239'>240</a>\u001b[0m     \u001b[39mif\u001b[39;00m wait_timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=240'>241</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=241'>242</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m (of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) futures unfinished\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=242'>243</a>\u001b[0m                 \u001b[39mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=244'>245</a>\u001b[0m waiter\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait(wait_timeout)\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=246'>247</a>\u001b[0m \u001b[39mwith\u001b[39;00m waiter\u001b[39m.\u001b[39mlock:\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/concurrent/futures/_base.py?line=247'>248</a>\u001b[0m     finished \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=571'>572</a>\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=572'>573</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=573'>574</a>\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=574'>575</a>\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=309'>310</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=310'>311</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=311'>312</a>\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=312'>313</a>\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home_local/dpenzar/miniconda3/lib/python3.9/threading.py?line=313'>314</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cfg in configs:\n",
    "    print(f\"Dataset {cfg.name}\")\n",
    "    ds_path = DS_DIR/f\"{protocol.get_shades_ds_name(cfg)}.bed\"\n",
    "    ds = ChIPSeqDataset.load(ds_path, tp=DatasetType.TRAIN, fmt='.tsv')\n",
    "    ds.name = protocol.get_shades_ds_name(cfg)\n",
    "    ds.tf_name = cfg.tf\n",
    "    \n",
    "    futures = {}\n",
    "    for model in bench.models:\n",
    "        #if not (isinstance(model, Model) or ds.tf_name is None or model.tfs is None or ds.tf_name in model.tfs):\n",
    "        if not ds.tf_name in model.tfs:\n",
    "            continue\n",
    "        ft = executor.submit(bench.score_model_on_ds, model, ds)\n",
    "        futures[ft] = f\"{model.name}@{ds.name}\"\n",
    "\n",
    "    print(f\"Submited {len(futures)} futures\")\n",
    "\n",
    "    for ft in as_completed(futures):\n",
    "        name = futures[ft]\n",
    "        path = ChIPSeqLOGS / f\"{name}.json\"\n",
    "        try:\n",
    "            scores = ft.result()\n",
    "            with path.open(\"w\") as outp:\n",
    "                json.dump(scores, outp)\n",
    "        except Exception as exc:\n",
    "            print(exc)\n",
    "            print(f\"Error for {name}\")\n",
    "        else:\n",
    "            print(f\"Finished for {name}\")\n",
    "\n",
    "    \n",
    "\n",
    "    ds_path = DS_DIR/f\"{protocol.get_foreign_ds_name(cfg)}.bed\"\n",
    "    ds = ChIPSeqDataset.load(ds_path, tp=DatasetType.TRAIN, fmt='.tsv')\n",
    "    ds.name = protocol.get_foreign_ds_name(cfg)\n",
    "    ds.tf_name = cfg.tf\n",
    "\n",
    "    futures = {}\n",
    "    for model in bench.models:\n",
    "        if not (isinstance(model, Model) or ds.tf_name is None or model.tfs is None or ds.tf_name in model.tfs):\n",
    "            continue\n",
    "        ft = executor.submit(bench.score_model_on_ds, model, ds)\n",
    "        futures[ft] = f\"{model.name}@{ds.name}\"\n",
    "\n",
    "    print(f\"Submited {len(futures)} futures\")\n",
    "    for ft in as_completed(futures):\n",
    "        name = futures[ft]\n",
    "        path = ChIPSeqLOGS / f\"{name}.json\"\n",
    "\n",
    "        try:\n",
    "            scores = ft.result()\n",
    "            with path.open(\"w\") as outp:\n",
    "                json.dump(scores, outp)\n",
    "        except Exception as exc:\n",
    "            print(f\"Error for {name}\")\n",
    "            print(exc)\n",
    "        else:\n",
    "            print(f\"Finished for {name}\")\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3363"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgb.autosome.ru/home_local/dpenzar/ibis-challenge/benchmark/1.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m \u001b[39msum\u001b[39m([e\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mvalue \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m ds_lst[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mentries]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(ds_lst[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mentries)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_lst' is not defined"
     ]
    }
   ],
   "source": [
    "sum([e.label.value for e in ds_lst[0].entries]) / len(ds_lst[0].entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= BedData.from_file(configs[0].path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169882"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pwmeval import PWMEvalPWMPredictor\n",
    "\n",
    "pfm_path = Path(\"/home_local/dpenzar/ibis-challenge/benchmark/example.pwm\")\n",
    "model = PWMEvalPWMPredictor.from_pfm(pfm_path, Path(\"/home_local/dpenzar/PWMEval/pwm_scoring\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.fasta  COPYING  Makefile     pwm_scoring.c  README.md   seqshuffle.c\n",
      "a.txt\t log.txt  pwm_scoring  README\t      seqshuffle\n"
     ]
    }
   ],
   "source": [
    "!ls /home_local/dpenzar/PWMEval/pwm_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = prepare_dataset(protocol, configs[0], sampler=ForeignPeakSampler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChIPSeqDataset(name='SI0159', type=<DatasetType.VALIDATION: 'val'>, tf_name='ZNF454', metainfo={})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol.preprocess(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bed = BedData.from_file('.ChIPSEQ_DB/total.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = BedData()\n",
    "for entry in total_bed:\n",
    "    if len(entry) != PEAK_SIZE: \n",
    "        if entry.peak is not None:\n",
    "            entry = BedEntry.from_center(entry.chr, entry.peak, radius=PEAK_SIZE//2)\n",
    "        else:\n",
    "            entry = BedEntry.from_center(entry.chr, (entry.start + entry.end) // 2, radius=PEAK_SIZE//2)\n",
    "    total_dataset.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_seqs = genome.cut(total_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a6b89ce9ffc256b1b7590cf8df7eee41f38232dcd839c1409bba0f1e7ed7c9d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
